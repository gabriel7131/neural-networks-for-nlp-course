{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction & Algebra 101 \n",
    "Before delving into the realm of neural networks, it is wise to familiarize ourselves with the tools we are going to be using. True hackers know the tools of their craft as the palm of their hands. \n",
    "\n",
    "We are first going to look at the mathematical operations that we will need to build neural networks. No degree of mathematical literacy is expected. However, fluency helps. To get you going as much as possible, we will be looking at graphical interpretations of almost anything we do. I find that playing with plots brings a childlike learning state that makes hard concepts much easier to grasp.\n",
    "\n",
    "Some level fluency with Python is assumed, though good programmers should be able to easily pick it up. Python's syntax is easy to read and comes bundled with thousands of libraries to make our life easier. In any case, we will start slow and start packing further. \n",
    "\n",
    "Without further ado, we begin preparing our necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math \n",
    "import numpy as np \n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is a library for efficient array processing, linear algebra and number crunching. It is the most common Python interface for working with vectors, matrices and tensors. Matplotlib is the go-to plotting library for Python. With these two libraries, we will show the basic ideas one by one. We also import iPython widgets so we interactively explore concepts later in this class. Let's see what all these words mean, a couple at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.random.random(2).astype(np.float32)\n",
    "matrix = np.random.random((3, 2))\n",
    "tensor = np.random.random((2, 3, 2))\n",
    "\n",
    "print('Printing vector:')\n",
    "print(vector)\n",
    "print('Printing matrix:')\n",
    "print(matrix)\n",
    "print('Printing tensor:')\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined three variables: \n",
    "\n",
    " * a **vector**, which is nothing more than an array of a given type with a fixed number of elements, usually called components, \n",
    " * a **matrix**, which can be understood as a list of vectors stacked together, and\n",
    " * a **tensor**, which generalizes vectors and matrices into typed structures with some number of dimensions and a fixed number of components in each dimension.\n",
    "\n",
    "In particular, we have a float32 vector with 2 components, a float64 matrix with $3 \\times 2$ components and a float64 tensor with $2 \\times 3 \\times 2$ components. Generally, we call `shape` the tuple representing the number of components per dimension. Tensors are typed: the matrix and the tensor are float64 because it is the default type that NumPy assumes. All the values are random and between 0 and 1, though at this we don't worry about the specific values. We are using relatively small tensors so that we can print them, but their number of components, their shapes, can be arbitrarily large. More on that in a moment.\n",
    "\n",
    "Let's look at some operations with tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_x_number = vector * 2\n",
    "print(vector)\n",
    "print(vector_x_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case we are multiplying by a number. Doing this will simply multiply each of the components by the number. Another way to see this is as 'scaling' the number. Addition, subtraction, division and power also work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector + 1)\n",
    "print(vector - 1)\n",
    "print(vector / 2)\n",
    "print(vector ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can actually be applied between tensors that are compatible. For instance, we can add the components of two vectors together, one by one. All basic operations are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector + vector) # should be the same as vector_x_number\n",
    "print(vector * vector)\n",
    "print(vector / vector) # should be (1, 1)\n",
    "print(vector ** vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape compatibility roughly means that we can align the tensors so that we can operate component by component. This is called a **pointwise** operation. Let's add our vector to our whole matrix as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix + vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interesting operation is the dot product. We will get an intuition of what it does later, but for now we can just understand it as an operation that takes two vectors of the same length and gives us a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_x_vector = vector.dot(vector)\n",
    "print(vector_x_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two vectors, the dot product is computed as the sum of products of each component. In standard, unoptimized python, this will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_example(a, b):\n",
    "    '''Computes the dot product between two vectors a and b'''\n",
    "    if a.size != b.size:\n",
    "        raise RuntimeError('Vector sizes must match!')\n",
    "    accum = 0.0\n",
    "    for i in range(a.size):\n",
    "        accum += a[i] * b[i]\n",
    "    return accum\n",
    "\n",
    "print(dot_example(vector, vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond some rounding error, we get the same result. NumPy is optimized under the hood so it is better for performance to stick to its functions if possible. Besides, our implementation is brittle and not very general! You can also compute the dot product between a vector and a matrix, or between matrices. When we do that, we have to make sure that the sizes of the matrices or vectors that we are working with match. What does this mean?\n",
    "\n",
    "When working with a vector, the sizes must match. In the case of a vector multiplying a matrix (or the other way around), the vector's size must match the size of the first component of the matrix. In general, for two matrices $A$ and $B$, we need to make sure that $A$ has shape $(N, M)$ and $B$ has shape $(M, L)$. Vectors are $(1, N)$ or $(N, 1)$ matrices when looking at it like this.\n",
    "\n",
    "If you are confused, just take a look at some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the vector: {}'.format(vector.shape))\n",
    "print('Shape of the matrix: {}'.format(matrix.shape))\n",
    "\n",
    "matrix_x_vector = matrix.dot(vector) # (3, 2) and (2, 1)\n",
    "vector_x_matrix = vector.dot(matrix) # comment this line out -- shapes don't match!\n",
    "\n",
    "print('Shape of the dot product: {}'.format(matrix_x_vector.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with matrices, the shape of the result in the dot product will be $(N, L)$. This is because for each of the $N$ vectors in the first matrix will have $L$ vectors in the second one, while $M$ will be the shared length of the vectors operating with one another. If we want to 'flip' a matrix, swapping columns by rows and viceversa, this is called transposing it. The transpose of $A$, with shape $(N, M)$, is $A^T$, with shape $(M, N)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_transpose = matrix.T # T gives us the transpose!\n",
    "print('Original matrix: \\n{}'.format(matrix))\n",
    "print('Transposed matrix: \\n{}'.format(matrix_transpose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the shapes now match, we can compute dot(A, A.T)!\n",
    "print('Notice that order matters & affects results!')\n",
    "print('--------------------------------------------')\n",
    "\n",
    "matrix_product = matrix.dot(matrix_transpose)\n",
    "print('Shape of dot(A, A^T): {}'.format(matrix_product.shape))\n",
    "matrix_product_tr = matrix_transpose.dot(matrix)\n",
    "print('Shape of dot(A^T, A): {}'.format(matrix_product_tr.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute dot products with general tensors as well. However, we don't need it to grasp the underlying principles and it can murk the initial intuition. If you want to look at it in depth, however, feel free to jump into NumPy's fantastic documentation:\n",
    "\n",
    "* [NumPy's np.dot documentation](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dot.html#numpy.dot)\n",
    "    \n",
    "At this point, we know all we need to manage neural networks! Of course, there is much more we can cover, but these basic notions of linear vector algebra are more than enough to get started. Our next step is to get a visual intuition of what we will be building. Geometry, here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planar Geometry for Neural Networks\n",
    "Neural Networks are easy to understand in geometric terms. Before starting with them, it makes sense to go through some definitions that give us some visual understanding. We will also try to get a good grasp of the concepts intuitively, playing with interactive plots. \n",
    "\n",
    "Our first step is to describe our terminology:\n",
    "\n",
    "* We work in an space with $N$ dimensions.\n",
    "* In our space, we will find points, defined as vectors with $N$ components.\n",
    "* Points in space will be separated by planes: 'sheets' defined by a vector with $N$ components that represents the direction of the plane and a bias variable which is just a number representing a 'shift'.\n",
    "\n",
    "The definition of the plane might seem a bit hairy. To fully grasp it, let's go step by step, first looking at points in 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.random((100, 2))\n",
    "plt.scatter(points[:, 0], points[:, 1], c='b')\n",
    "plt.gca().set_xlim(0, 1)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted 100 points generated at random in the interval from 0 to 1 on both dimensions. Each of the points has a vector, which represents it in space. Let's print only the first two vectors of our matrix and look at their positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points[:2, :])\n",
    "index_colors = ['r', 'g']\n",
    "colors = [index_colors[i] if i < 2 else 'b' for i in range(points.shape[0])]\n",
    "plt.scatter(points[:, 0], points[:, 1], c=colors)\n",
    "plt.gca().set_xlim(0, 1)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the coordinates and the positions in the plane it is simple to see which one is which: we just look at the axes and try to fathom which of the two red points falls closer to our coordinates. \n",
    "\n",
    "An important concept when dealing with points is the distance between them. To tackle this, we have to choose a way to actually measure that distance, a metric. Usually we will work with the euclidean distance, which is just the distance of a straight line going from one point to the other. Computing this is simple, as it just means finding the length of the largest side in a triangle—the hypothenuse. Let's see what we mean with the two highlighted points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_points = points[:2, :]\n",
    "two_colors = colors[:2]\n",
    "p1, p2 = two_points\n",
    "first_triangle = np.asarray([[0, 0], p1, p1 * (1, 0), [0, 0]])\n",
    "second_triangle = np.asarray([[0, 0], p2, p2 * (1, 0), [0, 0]])\n",
    "\n",
    "# plot points again, this time with the triangles\n",
    "plt.scatter(points[:, 0], points[:, 1], c=colors)\n",
    "plt.plot(first_triangle[:, 0], first_triangle[:, 1], c='r')\n",
    "plt.plot(second_triangle[:, 0], second_triangle[:, 1], c='g')\n",
    "plt.gca().set_xlim(0, 1)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can find the distance from the origin at position $(0, 0)$ to each of the highlighted points by simply getting the length of the vector. This length of a vector is also called its **norm**. NumPy provides a function, `np.linalg.norm` to do exactly this. Let's see how the length of a vector is implemented so you get a sense of what we are doing and then just focus on the bigger picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_example(v):\n",
    "    accum = 0.0\n",
    "    for c in v:\n",
    "        accum += c * c\n",
    "    return math.sqrt(accum)\n",
    "\n",
    "np_norm = np.linalg.norm(p1)\n",
    "example_norm = norm_example(p1)\n",
    "\n",
    "print('The norm computed with NumPy is {} while it is {} with our function.'.format(np_norm, example_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim here is to understand that the operations we are performing are really simple: the norm is nothing more than the square root of sum of each component, squared. This is called the L2-norm, as there can be other norms and ways of defining vector lengths. For us, however, this is all we need.\n",
    "\n",
    "Let's take a step back. The norm function we have defined should look very similar to the dot product we defined before. In fact, it seems to be the square root of the dot product of a vector with itself. Scroll up and check. Then, let's see if that also produces the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_norm = math.sqrt(p1.dot(p1))\n",
    "print('The square root of the dot product of the point with itself is {}: equal to what we computed!'.format(dot_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toolbox now contains 3 different ways of computing the norm of a vector. We would like to compute the distance between the red and green dot. Let's do another little drawing with triangles to see if we can gain some extra intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_shift = [p1[0] if p1[1] > p2[1] else p2[0], p2[1] if p2[1] < p1[1] else p1[1]]\n",
    "relative_triangle = np.asarray([p_shift, p1, p2, p_shift])\n",
    "\n",
    "# plot the three triangles without the points in the back\n",
    "plt.scatter(points[:2, 0], points[:2, 1], c=colors)\n",
    "plt.plot(first_triangle[:, 0], first_triangle[:, 1], c='r')\n",
    "plt.plot(second_triangle[:, 0], second_triangle[:, 1], c='g')\n",
    "plt.plot(relative_triangle[:, 0], relative_triangle[:, 1], c='m')\n",
    "plt.gca().set_xlim(0, 1)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it should be pretty clear: we want the norm of the vector between the two points! Luckily for us, this is just a pointwise operation: we subtract the point at the end from the point at the beginning. After that, we get the vector between the two. Of course, when computing the distance it does not matter which one goes first: direction has no effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_vector = p2 - p1\n",
    "plot_points = np.asarray([[0, 0], distance_vector, [distance_vector[0], 0], [0, 0]])\n",
    "\n",
    "# plot the vector relative to the red point\n",
    "plt.scatter(plot_points[:2, 0], plot_points[:2, 1], c=['r', 'g'])\n",
    "plt.plot(plot_points[:, 0], plot_points[:, 1], c='m')\n",
    "plt.axhline(0, color='k', linestyle='dotted', linewidth=0.5)\n",
    "plt.axvline(0, color='k', linestyle='dotted', linewidth=0.5)\n",
    "plt.gca().set_xlim(-1, 1)\n",
    "plt.gca().set_ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the segment we want to compute the length of, represented as the vector going from the red point to the green one. The little magenta triangle represents that same path, but as if it sat in the center of the universe. The distance between $P_1$ and $P_2$ is the norm of the distance vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The distance between p1 and p2 is {}.'.format(np.linalg.norm(distance_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are almost done with our geometric primitives. We are just missing the concept of planes, which will let us linearly split space. This, in turn, will be the base for our introduction to neural networks. Let's begin by looking at points in a single dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = np.random.random((1000)) * 10 - 5\n",
    "flats_y = flats * 0 # y component is always 0 in this little plot\n",
    "c = ['r' if p < 0 else 'b' for p in flats]\n",
    "plt.scatter(flats, flats_y, marker='.', c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot, negative and positive points are colored differently. Our points go from -5 to 5, and we would like to arbitrarily set the boundary at any point in the number line, as well as arbitrarily chosing which side is red or blue. \n",
    "\n",
    "With some thinking, we can see that if we change the sign in the number line, we can get our color change. That is, we just multiply a number, which we call weight, by the point: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(s):\n",
    "    '''Display a line of points and the effect of separating them using a weight variable.'''\n",
    "    c_weight = ['r' if s * p < 0 else 'b' for p in flats]\n",
    "    plt.scatter(flats, flats_y, marker='.', c=c_weight)\n",
    "    plt.show()\n",
    "\n",
    "interact(update_weight, \n",
    "         s=widgets.Select(options=[-2, -1, 0, 1, 2], value=1, description='Weight: '));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive weights keep blue in the positive and negative ones invert the colors so red is on the right and blue on the left. The actual numeric value of the weight doesn't matter, because our boundary only takes the sign into account.\n",
    "\n",
    "Likewise, if we add some value to each of the numbers, we can shift them so that the colors are applied with some sort of shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias(b):\n",
    "    '''Display a line of points and the effect of separating them using of a bias variable.'''\n",
    "    c_bias = ['r' if p + b < 0 else 'b' for p in flats]\n",
    "    plt.scatter(flats, flats_y, marker='.', c=c_bias)\n",
    "    plt.show()\n",
    "\n",
    "interact(update_bias, \n",
    "         b=widgets.FloatText(value=0.0, step=0.1, description='Bias: '));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the values in both interactive plots. We want to combine both ideas into one single decision, one single formula. The solution is to include both the weight and bias variables.\n",
    "\n",
    "If you are quick with math, you might have noticed that this resembles representing our color as the sign of $f(x) = Ax + B$. The sign of $A$ will determine the direction of coloring the points, while $B$ will act as a shift that moves us around. If the sign of $f(x)$ is negative, we will assign red; otherwise blue. The following interactive plot should compliment these ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_full(s, b):\n",
    "    '''Display a line of points and the effect of separating them using both weight and bias variables.'''\n",
    "    c_full = ['r' if s * p + b < 0 else 'b' for p in flats]\n",
    "    plt.scatter(flats, flats_y, marker='.', c=c_full)\n",
    "    plt.show()\n",
    "\n",
    "interact(update_full, \n",
    "         s=widgets.FloatText(value=1.0, step=0.1, description='Weight: '),\n",
    "         b=widgets.FloatText(value=0.0, step=0.1, description='Bias: '));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have what we were looking for, but it's not enough. First of all, the real world is not one dimensional! \n",
    "\n",
    "In a more general case, we have an arbitrary number of components in an input vector $v$, $f(v) = w_1 v_1 + w_2 v_2 + ... + w_n v_n + b$. This operation, if you are keeping score at home, is just the dot product of $v$ with a weight vector $w$, to which we add $b$, a bias variable. Those names are funky but they will come to be part of our usual vocabulary. Don't get scared!\n",
    "\n",
    "To get a better grasp of what we mean, we will interactively look at a two dimensional example of the same idea. Here, we have two components for our vector: one on the X axis and another one on the Y axis. Then, just like in the previous case, we have our little bias variable. Play around and you will notice how, with some effort, you can separate some of the points at will:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(-3.5, 3.5)\n",
    "X = np.random.random((10000, 2)) * 7 - 3.5\n",
    "def plot_hyperplane(x, y, c):\n",
    "    '''Plot a plane with weights x and y and bias c.'''\n",
    "    if y:\n",
    "        a = -x / y\n",
    "        yy = a * xx - c / y\n",
    "        plt.plot(xx, yy, 'k-')\n",
    "    else:\n",
    "        s = np.linalg.norm([x, y])\n",
    "        plt.vlines([-c / s], ymin=-3.5, ymax=3.5, colors='k')\n",
    "    plt.show()\n",
    "\n",
    "def plot_space(v, b):\n",
    "    '''Plot the points taking into account the vector of weights v = (x, y) and the bias b.'''\n",
    "    P = np.dot(X, v) + b \n",
    "    c = ['r' if p < 0 else 'b' for p in P]\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker='.', c=c)\n",
    "    \n",
    "def update_2d(u, w, b):\n",
    "    '''Display the effect of the weight vector (x, y) and the bias variable.'''\n",
    "    v = [u, w]\n",
    "    plot_space(v, b)\n",
    "    plt.gca().set_xlim(-3.5, 3.5)\n",
    "    plt.gca().set_ylim(-3.5, 3.5)\n",
    "    plot_hyperplane(u, w, b)\n",
    "    print('The hyperplane is formed by the direction vector {}, with length {}, and a bias variable of {}.'.format(v, np.linalg.norm(v), b))\n",
    "\n",
    "    \n",
    "interact(update_2d, \n",
    "         u=widgets.FloatText(value=1.0, step=0.1, description='X comp: '),\n",
    "         w=widgets.FloatText(value=0.0, step=0.1, description='Y comp: '),\n",
    "         b=widgets.FloatText(value=0.0, step=0.1, description='Bias: '));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good exercise is to try to separate some part of our space. For instance, play around until you find a plane that colors everything but the top left in red. That is, values that match a plot similar to:\n",
    "\n",
    "![Sample problem](images/TargetPlane.png)\n",
    "\n",
    "When you find the correct values, put them in the next cell to validate them. If they are correct, you will get a `Correct!` message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "started = False\n",
    "def validate(u, w, b):\n",
    "    '''Validate a given plane setup for classifying the points as shown in the image.'''\n",
    "    if not started:\n",
    "        print('Find the correct values to replicate the plot above in the interactive plot!')\n",
    "        return\n",
    "    \n",
    "    # validation plane\n",
    "    a = 3 * np.pi / 4\n",
    "    b_or = -3.1819805153\n",
    "    v_or = [np.cos(a), np.sin(a)]\n",
    "    P_or = np.dot(X, v_or) + b_or\n",
    "    \n",
    "    # user-input plane\n",
    "    v_in = [u, w]\n",
    "    P_in = np.dot(X, v_in) + b\n",
    "    \n",
    "    # verification tuples\n",
    "    ver = zip(*[[p < 0 for p in P] for P in [P_or, P_in]])\n",
    "    total = float(sum(a == b for (a, b) in ver))\n",
    "    \n",
    "    # if 95% of the points are correctly classified it's fine enough!\n",
    "    total_correct = total / len(P_or)\n",
    "    if total_correct > 0.95:\n",
    "        print('Correct! The plane defined by {} and {} is similar to the one above (with {}% accuracy)!'.format(v_in, b, total_correct * 100))\n",
    "        update_2d(u, w, b)\n",
    "    else:\n",
    "        print('Nope! Your results are too different from the above plot, try narrowing it down!')\n",
    "\n",
    "interact(validate, \n",
    "         u=widgets.FloatText(value=1.0, step=0.1, description='X comp: '),\n",
    "         w=widgets.FloatText(value=0.0, step=0.1, description='Y comp: '),\n",
    "         b=widgets.FloatText(value=0.0, step=0.1, description='Bias: '));\n",
    "started = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this has given you an understanding of what a plane is: nothing but a direction shifted by some bias! With it, we have seen we can partition parts of space. Furthermore, the way we compute whether or not a point is in front of them is by performing the dot product of the direction vector and then adding up the bias. If the result is positive, we are in front; we are in the back if it is negative and in the middle of the plane if it is exactly 0.\n",
    "\n",
    "To recap all that we have seen so far, let's remind ourselves of the two sections and their contents:\n",
    "1. On Algebra 101 we covered:\n",
    "    1. NumPy arrays: vectors, matrices and tensors\n",
    "    2. Scalar operations on those arrays\n",
    "    3. Point-wise operations between tensors\n",
    "    4. The infamous dot product!\n",
    "2. For our Geometric Primitives, we discussed:\n",
    "    1. Points in space\n",
    "    2. Vectors between points, their meaning and their length\n",
    "    3. Distances between points\n",
    "    4. Planes, in both 1 and 2 dimensions, and the boundaries they create!\n",
    "\n",
    "In the next class, we will finally start with neural networks. However, let's get a little taste. Consider planes as we have seen so far: can you think of a way to separate the red and blue areas shown in the next plot with them?\n",
    "\n",
    "![Mental exercise: separate a parabola](images/TargetSeparate.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
